접속 정보

ssh root@localhost -p 60010

ssh root@localhost -p 60101

ssh root@localhost -p 60102

ssh root@localhost -p 60103

vagrant







# 쿠버네티스 구성요소

### [master]

kubectl get node



## kubectl config

### [worker3]

 scp root@192.168.1.10:/etc/kubernetes/admin.conf .

[root@w3-k8s ~]# ll
합계 12
-rw-------. 1 root root 5452 10월 22 22:09 admin.conf
-rw-------. 1 root root 1377  9월 15  2019 anaconda-ks.cfg



[root@w3-k8s ~]# kubectl get nodes --kubeconfig admin.conf
NAME     STATUS   ROLES    AGE   VERSION
m-k8s    Ready    master   46m   v1.18.4
w1-k8s   Ready    <none>   43m   v1.18.4
w2-k8s   Ready    <none>   40m   v1.18.4
w3-k8s   Ready    <none>   37m   v1.18.4







## kubelet


### [master]

[root@m-k8s 3.1.6]# pwd
/root/_Book_k8sInfra/ch3/3.1.6
[root@m-k8s 3.1.6]# kubectl apply -f nginx-pod.yaml
pod/nginx-pod create

[root@m-k8s 3.1.6]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   0          18s
[root@m-k8s 3.1.6]# kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
nginx-pod   1/1     Running   0          26s   172.16.221.129   w1-k8s   <none>           <none>



보면 worker1에 존재하는 것을 확인

worker1로 접속해서 kubelet 서비스를 멈춤



### [worker1]

[root@w1-k8s ~]# systemctl stop kubelet





다시 마스터 노드에 접속해서 삭제함.

### [master]

[root@m-k8s 3.1.6]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   0          5m8s
[root@m-k8s 3.1.6]# kubectl delete pod nginx-pod
pod "nginx-pod" deleted





하지만 시간이 오래 지나도록 삭제되지 않는 것을 확인할 수 있음 control c를 눌러 취소하고 파드 상태 확인



NAME        READY   STATUS        RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
nginx-pod   1/1     Terminating   0          9m41s   172.16.221.129   w1-k8s   <none>           <none>



보면 터미네이팅 되고 있지만 삭제가 안된다 이유는 위치하고 있는 worker1의 kubelet 이 동작을 멈췄기 때문. 선장이 부재중이라 pod에게 명령을 전달할 오브젝트가 없다. 다시 워커1으로 가서 kubelet을 올림



### [worker1]

[root@w1-k8s ~]# systemctl start kubelet



### [master]

마스터에서 다시 파드를 확인해보면

[root@m-k8s 3.1.6]# kubectl get pods -o wide
NAME        READY   STATUS        RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
nginx-pod   0/1     Terminating   0          12m   172.16.221.129   w1-k8s   <none>           <none>
[root@m-k8s 3.1.6]# kubectl get pods -o wide
No resources found in default namespace.



삭제되고 있는 것을 확인할 수 있다.



kubelet은 파드의 상태를 관리한다.



## kubeproxy

이번에는 쿠베 프록시를 알아보자



### [master]

[root@m-k8s ~]# cd _Book_k8sInfra/ch3/3.1.6
[root@m-k8s 3.1.6]# ls
nginx-pod.yaml
[root@m-k8s 3.1.6]# kubectl apply -f nginx-pod.yaml
pod/nginx-pod created
[root@m-k8s 3.1.6]# kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
nginx-pod   1/1     Running   0          30s   172.16.103.129   w2-k8s   <none>           <none>



```
[root@m-k8s 3.1.6]# curl 172.16.103.129
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```



### [worker2]

워커노드 2에서 br_netfiter 명령으로 파드가 위치한 워커노드에서 br_netfiter  삭제



[root@w2-k8s ~]# modprobe -r br_netfilter

[root@w2-k8s ~]# systemctl restart network



### [master] 

에서 다시 아까 페이지 호출

[root@m-k8s 3.1.6]# curl 172.16.103.129
curl: (7) Failed connect to 172.16.103.129:80; 연결 시간 초과



호출안되는 것 확인

이유 : 쿠베프록시가 이용하는 br_netfilter 가 문제있어서 이다



다시 원복

### [worker2]

[root@w2-k8s ~]# modprobe  br_netfilter

[root@w2-k8s ~]# reboot







### [master]

[root@m-k8s 3.1.6]# kubectl get pods
NAME        READY   STATUS      RESTARTS   AGE
nginx-pod   0/1     Completed   0          9m54s
[root@m-k8s 3.1.6]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   1          10m





몇분있다 확인하면 다시 올라온걸 확인 호출



[root@m-k8s 3.1.6]# kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
nginx-pod   1/1     Running   1          10m   172.16.103.130   w2-k8s   <none>           <none>





```root@m-k8s 3.1.6]# curl 172.16.103.130
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@m-k8s 3.1.6]#
```



삭제후 다음 작업

[root@m-k8s 3.1.6]# kubectl delete -f nginx-pod.yaml
pod "nginx-pod" deleted





# Pod

## run 생성

### [master]

[root@m-k8s 3.1.6]# kubectl run nginx-pod --image=nginx
pod/nginx-pod created

[root@m-k8s 3.1.6]# kubectl get pod
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   0          12s



## craete 생성

### [master]

[root@m-k8s 3.1.6]# kubectl create nginx --image=nginx
Error: unknown flag: --image
See 'kubectl create --help' for usage



이대로 하면 안됨 create 는 deploy 로 생성

[root@m-k8s 3.1.6]# kubectl create deploy dpy-nginx --image=nginx
deployment.apps/dpy-nginx created



[root@m-k8s 3.1.6]# kubectl get deploy
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
dpy-nginx   1/1     1            1           29s
[root@m-k8s 3.1.6]# kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
dpy-nginx-c8d778df-fkhqc   1/1     Running   0          34s
nginx-pod                  1/1     Running   0          2m52s







## scale



deployment 없이 배포한 것은 scale  up 하지 못함

[root@m-k8s 3.1.6]# kubectl scale pod nginx-pod --replicas=3
Error from server (NotFound): the server could not find the requested resource



deployment 있게 배포 한것은  scale 됨

[root@m-k8s 3.1.6]# kubectl scale deploy dpy-nginx  --replicas=3
deployment.apps/dpy-nginx scaled
[root@m-k8s 3.1.6]# kubectl get pods
NAME                       READY   STATUS              RESTARTS   AGE
dpy-nginx-c8d778df-5s7cd   0/1     ContainerCreating   0          7s
dpy-nginx-c8d778df-fkhqc   1/1     Running             0          9m57s
dpy-nginx-c8d778df-r5bwh   1/1     Running             0          7s

[root@m-k8s 3.1.6]# kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
dpy-nginx-c8d778df-5s7cd   1/1     Running   0          48s   172.16.132.3     w3-k8s   <none>           <none>
dpy-nginx-c8d778df-fkhqc   1/1     Running   0          10m   172.16.221.130   w1-k8s   <none>           <none>
dpy-nginx-c8d778df-r5bwh   1/1     Running   0          48s   172.16.103.132   w2-k8s   <none>           <none>
nginx-pod                  1/1     Running   0          12m   172.16.103.131   w2-k8s   <none>           <none>



[root@m-k8s 3.1.6]# kubectl delete deploy dpy-nginx
deployment.apps "dpy-nginx" deleted
[root@m-k8s 3.1.6]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   0          13mspec:  replicas: 1  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: echo-hname        image: sysnet4admin/echo-hname ~







## yaml 파일

echo-hname.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo-hname
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: echo-hname
        image: sysnet4admin/echo-hname
```



[root@m-k8s 3.2.4]# kubectl apply -f echo-hname.yaml
deployment.apps/echo-hname created



yaml 파일  수정

spec:
  replicas: 1
  selector:



하나로 바뀜

[root@m-k8s 3.2.4]# kubectl apply -f echo-hname.yaml
deployment.apps/echo-hname configured
[root@m-k8s 3.2.4]# kubectl get pods
NAME                        READY   STATUS        RESTARTS   AGE
echo-hname-7894b67f-7jlqr   1/1     Running       0          67s
echo-hname-7894b67f-ll85c   0/1     Terminating   0          67s



## version

사용가능한 api 버전 확인



[root@m-k8s 3.2.4]# kubectl api-versions
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps/v1
authentication.k8s.io/v1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1
authorization.k8s.io/v1beta1
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1beta1
coordination.k8s.io/v1
coordination.k8s.io/v1beta1
crd.projectcalico.org/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1beta1
extensions/v1beta1
networking.k8s.io/v1
networking.k8s.io/v1beta1
node.k8s.io/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1
scheduling.k8s.io/v1
scheduling.k8s.io/v1beta1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1





## exec 

[root@m-k8s 3.2.4]# kubectl exec -it nginx-pod -- /bin/bash



-- 은 인자값을 나누고 싶을 때 사용

[root@m-k8s 3.2.4]# kubectl exec -it nginx-pod -- ls -l /run
total 4
drwxrwxrwt. 2 root root  6 Oct 11 00:00 lock
-rw-r--r--. 1 root root  2 Oct 22 15:12 nginx.pid
drwxr-xr-x. 4 root root 39 Oct 22 15:12 secrets
-rw-rw-r--. 1 root utmp  0 Oct 11 00:00 utmp



## 쿠버네티스 자동복구



[root@m-k8s 3.2.4]# kubectl get pods -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
echo-hname-7894b67f-7jlqr   1/1     Running   0          14m     172.16.132.4     w3-k8s   <none>           <none>
nginx-pod                   1/1     Running   0          6m44s   172.16.103.134   w2-k8s   <none>



root@nginx-pod:/# cat /run/nginx.pid
1



시간 확인

root@nginx-pod:/# ls -l /run/nginx.pid
-rw-r--r--. 1 root root 2 Oct 22 15:12 /run/nginx.pid



다른 창하나 키우고

[root@m-k8s ~]# i=1; while true; do sleep; echo $((i++)) `curl --silent 172.16.103.134 | grep title`; done

```
sleep: missing operand
Try 'sleep --help' for more information.
12557 <title>Welcome to nginx!</title>
sleep: missing operand
Try 'sleep --help' for more information.
```



반복 되는 거 확인



다른 창에서

root@nginx-pod:/# kill 1
root@nginx-pod:/# command terminated with exit code 137



root@nginx-pod:/#  ls -l /run/nginx.pid
-rw-r--r--. 1 root root 2 Oct 22 15:21 /run/nginx.pid



시간이 업데이트 됨

생성된 시간 확인





# cordon vs drain vs taint

* cordon : 지정된 노드에 더 이상 파드들이 스케줄링 되어서 실행되지 않도록 함

* drain : 지정한 노드에 있는 파드를 쫒아내고 스케줄링 되지 않게함
* taint :  지정한 노드에 포드들이 스케줄링 안되게 하고 만약 특정 파드를 스케줄링 되게 하고 싶음 toleration 사용



## cordon

: 문제가 생길 가능성이 있는 노드인지 판단



[root@m-k8s 3.2.4]# kubectl apply -f echo-hname.yaml
deployment.apps/echo-hname created
[root@m-k8s 3.2.4]# kubectl scale deploy echo-hname --replicas=9
deployment.apps/echo-hname scaled



[root@m-k8s 3.2.4]# kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName
NAME                        IP               STATUS    NODE
echo-hname-7894b67f-2f598   172.16.103.135   Running   w2-k8s
echo-hname-7894b67f-6hbdr   172.16.103.137   Running   w2-k8s
echo-hname-7894b67f-j7rld   172.16.132.7     Running   w3-k8s
echo-hname-7894b67f-kbk9j   172.16.221.134   Running   w1-k8s
echo-hname-7894b67f-lnw42   172.16.132.6     Running   w3-k8s
echo-hname-7894b67f-szsr7   172.16.103.136   Running   w2-k8s
echo-hname-7894b67f-tkc89   172.16.221.135   Running   w1-k8s
echo-hname-7894b67f-v85n9   172.16.132.5     Running   w3-k8s
echo-hname-7894b67f-vjbj4   172.16.221.133   Running   w1-k8s





노드에 파드 할당 되지 않게 설정 : 현재 상태 보존

[root@m-k8s ch3]# kubectl cordon w3-k8s
node/w3-k8s cordoned



[root@m-k8s ch3]# kubectl uncordon w3-k8s
node/w3-k8s uncordoned









## drain

근데 데몬셋을 지울수 없어서 명령을 실행할 수 없다고 나옴. drain은 실제로 파드를 옮기는 것이 아니라 노드에서 파드를 삭제하고 다른 곳에서 생성하는 것이다. 근데 데못셋은 각 노드에 1개만 존재하는 거라  drain으로 삭제할 수 없다.







[root@m-k8s ch3]# kubectl drain w3-k8s
node/w3-k8s already cordoned
error: unable to drain node "w3-k8s", aborting command...

There are pending nodes to be drained:
 w3-k8s
cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override): default/nginx-pod
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-9sxmv, kube-system/kube-proxy-pjjvf





그래서 ignore-daemonsets 옵션과 같이 사용해야 한다.



[root@m-k8s ch3]# kubectl drain w3-k8s --ignore-daemonsets --force
node/w3-k8s already cordoned
WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/nginx-pod; ignoring DaemonSet-managed Pods: kube-system/calico-node-9sxmv, kube-system/kube-proxy-pjjvf
evicting pod default/dpy-nginx-c8d778df-6rzd2
evicting pod default/nginx-pod
evicting pod default/echo-hname-7894b67f-lnw42
evicting pod default/echo-hname-7894b67f-v85n9
pod/echo-hname-7894b67f-v85n9 evicted
pod/nginx-pod evicted
pod/echo-hname-7894b67f-lnw42 evicted
pod/dpy-nginx-c8d778df-6rzd2 evicted
node/w3-k8s evicted





[root@m-k8s ch3]#  kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName
NAME                        IP               STATUS    NODE
dpy-nginx-c8d778df-6vg2x    172.16.103.138   Running   w2-k8s
dpy-nginx-c8d778df-jlrzb    172.16.221.137   Running   w1-k8s
dpy-nginx-c8d778df-lbv8w    172.16.221.138   Running   w1-k8s
echo-hname-7894b67f-2f598   172.16.103.135   Running   w2-k8s
echo-hname-7894b67f-4jjhz   172.16.221.140   Running   w1-k8s
echo-hname-7894b67f-mppm5   172.16.221.139   Running   w1-k8s
nginx                       172.16.103.139   Running   w2-k8s







[root@m-k8s ch3]# kubectl get node
NAME     STATUS                     ROLES    AGE   VERSION
m-k8s    Ready                      master   15h   v1.18.4
w1-k8s   Ready                      <none>   15h   v1.18.4
w2-k8s   Ready                      <none>   15h   v1.18.4
w3-k8s   Ready,SchedulingDisabled   <none>   15h   v1.18.4





다시 원복

[root@m-k8s ch3]# kubectl uncordon w3-k8s
node/w3-k8s uncordoned
[root@m-k8s ch3]# kubectl get node
NAME     STATUS   ROLES    AGE   VERSION
m-k8s    Ready    master   15h   v1.18.4
w1-k8s   Ready    <none>   15h   v1.18.4
w2-k8s   Ready    <none>   15h   v1.18.4
w3-k8s   Ready    <none>   15h   v1.18.4









# Deployment

## --record



[root@m-k8s 3.2.10]# pwd
/root/_Book_k8sInfra/ch3/3.2.10



[root@m-k8s 3.2.10]# kubectl apply  -f rollout-nginx.yaml --record
deployment.apps/rollout-nginx created





### rollout history

[root@m-k8s 3.2.10]# kubectl rollout history deploy rollout-nginx
deployment.apps/rollout-nginx
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=rollout-nginx.yaml --record=true



롤 아웃 히스토리 확인



NAME                             IP               STATUS    NODE
rollout-nginx-64dd56c7b5-kvmp5   172.16.103.142   Running   w2-k8s
rollout-nginx-64dd56c7b5-ndtj9   172.16.132.13    Running   w3-k8s
rollout-nginx-64dd56c7b5-pjvxm   172.16.221.142   Running   w1-k8s





[root@m-k8s 3.2.10]# curl -I --silent 172.16.103.142 | grep Server
Server: nginx/1.15.12





## set image

kubectl set image deploy rollout-nginx nginx=nginx:1.16.0 --record
deployment.apps/rollout-nginx image updated



[root@m-k8s 3.2.10]# kubectl get pods
NAME                             READY   STATUS              RESTARTS   AGE
rollout-nginx-64dd56c7b5-ndtj9   1/1     Running             0          7m
rollout-nginx-64dd56c7b5-pjvxm   0/1     Terminating         0          7m
rollout-nginx-8566d57f75-8cdhn   1/1     Running             0          34s
rollout-nginx-8566d57f75-psj8n   1/1     Running             0          17s
rollout-nginx-8566d57f75-xmzs9   0/1     ContainerCreating   0          3s





[root@m-k8s 3.2.10]# kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
rollout-nginx-8566d57f75-8cdhn   1/1     Running   0          56s
rollout-nginx-8566d57f75-psj8n   1/1     Running   0          39s
rollout-nginx-8566d57f75-xmzs9   1/1     Running   0          25s



[root@m-k8s 3.2.10]# curl -I --silent 172.16.103.143 | grep Server



Server: nginx/1.16.0



### rollout status

[root@m-k8s 3.2.10]# kubectl rollout status deploy rollout-nginx
deployment "rollout-nginx" successfully rolled out



[root@m-k8s 3.2.10]# kubectl rollout history deploy rollout-nginx
deployment.apps/rollout-nginx
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=rollout-nginx.yaml --record=true
2         kubectl set image deploy rollout-nginx nginx=1.16.0 --record=true      --> 오타났음
3         kubectl set image deploy rollout-nginx nginx=nginx:1.16.0 --record=true







만약 실수로 잘못된 버전으로 업데이트 한다면? pending상태로 멈춰있는것을 확인할 수 있음

[root@m-k8s 3.2.10]# kubectl set image deploy rollout-nginx nginx=1.17.23 --record
deployment.apps/rollout-nginx image updated
[root@m-k8s 3.2.10]# node
NAME                             IP               STATUS    NODE
rollout-nginx-7998b867d5-ml6x4   172.16.221.145   Pending   w1-k8s
rollout-nginx-8566d57f75-8cdhn   172.16.103.143   Running   w2-k8s
rollout-nginx-8566d57f75-psj8n   172.16.221.144   Running   w1-k8s
rollout-nginx-8566d57f75-xmzs9   172.16.132.14    Running   w3-k8s



상태 확인해보면 waiting

[root@m-k8s 3.2.10]# kubectl rollout status deploy rollout-nginx
Waiting for deployment "rollout-nginx" rollout to finish: 1 out of 3 new replicas have been updated...







## describe

: 쿠버네티스 상태 확인



kubectl describe deploy rollout-nginx



정상적이게 복구





## rollout undo





[root@m-k8s 3.2.10]# kubectl rollout undo deploy rollout-nginx
deployment.apps/rollout-nginx rolled back

[root@m-k8s 3.2.10]# kubectl rollout history deploy rollout-nginx
deployment.apps/rollout-nginx
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=rollout-nginx.yaml --record=true
2         kubectl set image deploy rollout-nginx nginx=1.16.0 --record=true
4         kubectl set image deploy rollout-nginx nginx=1.17.23 --record=true
5         kubectl set image deploy rollout-nginx nginx=nginx:1.16.0 --record=true





[root@m-k8s 3.2.10]# curl -I --silent 172.16.103.143 | grep Server
Server: nginx/1.16.0





## rollout undo --to-revision=1

특정 시점으로 롤백하기



[root@m-k8s 3.2.10]# kubectl rollout undo deploy rollout-nginx --to-revision=1
deployment.apps/rollout-nginx rolled back



[root@m-k8s 3.2.10]# node
NAME                             IP               STATUS    NODE
rollout-nginx-64dd56c7b5-5xvhs   172.16.132.15    Running   w3-k8s
rollout-nginx-64dd56c7b5-6krxb   172.16.221.146   Running   w1-k8s
rollout-nginx-64dd56c7b5-zt26c   172.16.103.144   Running   w2-k8s
[root@m-k8s 3.2.10]# curl -I --silent 172.16.132.15 | grep Server
Server: nginx/1.15.12





# service

: 쿠버네티스 연결 담당, 쿠버네티스 외부에서 쿠버네티스 클러스터 접속하는 방법



## nodeport

포트를 중복으로 사용할 수 없어서 1개의 노드포트 당 1개의 deployment 만 적용된다



[root@m-k8s 3.3.1]# kubectl delete deploy np-pod
deployment.apps "np-pod" deleted
[root@m-k8s 3.3.1]#  kubectl create deploy np-pods --image=sysnet4admin/echo-hname
deployment.apps/np-pods created
[root@m-k8s 3.3.1]# kubectl get deploy
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
np-pods   1/1     1            1           11s
[root@m-k8s 3.3.1]# kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
np-pods-5767d54d4b-9h2gr   1/1     Running   0          15s





[root@m-k8s 3.3.1]# kubectl apply -f nodeport.yaml
service/np-svc created



nodeport.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: np-svc
spec:
  selector:
    app: np-pods
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30000
  type: NodePort
```





[root@m-k8s 3.3.1]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        16h
np-svc       NodePort    10.101.70.76   <none>        80:30000/TCP   71s



워크 노드 아이피 확인



[root@m-k8s 3.3.1]# kubectl get node -o wide
NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
m-k8s    Ready    master   16h   v1.18.4   192.168.1.10    <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1
w1-k8s   Ready    <none>   16h   v1.18.4   192.168.1.101   <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1
w2-k8s   Ready    <none>   16h   v1.18.4   192.168.1.102   <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1
w3-k8s   Ready    <none>   15h   v1.18.4   192.168.1.103   <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1



pc 접속

![image-20211023134651505](../../Desktop/k8s/study-k8s/k8s-.assets/image-20211023134651505.png)

## scale

[root@m-k8s ~]# kubectl scale deploy np-pods --replicas=3
deployment.apps/np-pods scaled
[root@m-k8s ~]# kubectl get deploy
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
np-pods   3/3     3            3           5d10h





## expose

: 노드 포트 서비스 생성하기. 노트포트 서비스는 오브젝트 스펙 파일 뿐만 아니라 서비스 expose 명령어로도 생성할 수 있다.





[root@m-k8s ~]# kubectl expose deploy np-pods --type=NodePort --name=np-svc-v2 --port=80
service/np-svc-v2 exposed
[root@m-k8s ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        6d2h
np-svc       NodePort    10.101.70.76   <none>        80:30000/TCP   5d10h
np-svc-v2    NodePort    10.109.25.53   <none>        80:31939/TCP   13s



![image-20211029000855798](k8s-3장.assets/image-20211029000855798.png)





# Ingress

여러개의 deployment 가 있을 때 Ingress는 고유한 주소를 제공해 사용 목적에 따라 다른 응답을 제공할 수 있고, 트래픽에 대한 L4, L7 로드 벨런서 뿐만 아니라 보안 인증서를 처리하는 기능을 가지고 있다.

인그레스를 사용하려면 인그레스 컨트롤러가 필요하다. 

또한 인그레스 컨트롤러는 파드와 직접 통신할 수 없어서 노드 포트 혹은 로드 벨런서와 연결되어야 한다.



* 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 이동한다. 이때 노드 포트 서비스는 노드 nginx 인그레스 컨트롤러로 구성
* nginx 인그레스 컨트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공해준다
* 클러스터 IP 서비스는 사용자를 해당 파드로 연결 시킨다



[root@m-k8s ch3]# kubectl create deploy in-hname-pod --image=sysnet4admin/echo-hname
deployment.apps/in-hname-pod created
[root@m-k8s ch3]#
[root@m-k8s ch3]# kubectl create deploy in-ip-pod --image=sysnet4admin/echo-ip
deployment.apps/in-ip-pod created





[root@m-k8s ch3]# kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
in-hname-pod-8565c86448-glxmg   1/1     Running   0          94s
in-ip-pod-76bf6989d-9pbhd       1/1     Running   0          55s





```
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      # wait up to five minutes for the drain of connections
      terminationGracePeriodSeconds: 300
      serviceAccountName: nginx-ingress-serviceaccount
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 101
            runAsUser: 101
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown

---

apiVersion: v1
kind: LimitRange
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  limits:
  - min:
      memory: 90Mi
      cpu: 100m
    type: Container
```







[root@m-k8s 3.3.2]# kubectl apply -f ingress-nginx.yaml
namespace/ingress-nginx created
configmap/nginx-configuration created
configmap/tcp-services created
configmap/udp-services created
serviceaccount/nginx-ingress-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created
role.rbac.authorization.k8s.io/nginx-ingress-role created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created
deployment.apps/nginx-ingress-controller created
limitrange/ingress-nginx created







[root@m-k8s 3.3.2]# kubectl apply -f ingress-config.yaml
ingress.networking.k8s.io/ingress-nginx created
[root@m-k8s 3.3.2]# kubectl get pods -n ingress-nginx
NAME                                        READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-5bb8fb4bb6-w4tdv   1/1     Running   0          102s







[root@m-k8s 3.3.2]# kubectl get ing
NAME            CLASS    HOSTS   ADDRESS   PORTS   AGE
ingress-nginx   <none>   *                 80      42s





[root@m-k8s 3.3.2]# kubectl get ing -o yaml







[root@m-k8s 3.3.2]# kubectl apply -f ingress.yaml
service/nginx-ingress-controller created
[root@m-k8s 3.3.2]# kubectl get svc -n ingress-nginx
NAME                       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
nginx-ingress-controller   NodePort   10.100.126.27   <none>        80:30100/TCP,443:30101/TCP   3s







[root@m-k8s 3.3.2]# kubectl expose deploy in-hname-pod --name=hname-svc-default --port=80,443
service/hname-svc-default exposed





[root@m-k8s 3.3.2]# kubectl get svc
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hname-svc-default   ClusterIP   10.96.182.107   <none>        80/TCP,443/TCP   16s
kubernetes          ClusterIP   10.96.0.1       <none>        443/TCP          7d





![image-20211029221203033](k8s-3장.assets/image-20211029221203033.png)



[root@m-k8s 3.3.2]# kubectl delete deploy in-hname-pod
deployment.apps "in-hname-pod" deleted
[root@m-k8s 3.3.2]# kubectl delete deploy in-ip-pod
deployment.apps "in-ip-pod" deleted





[root@m-k8s 3.3.2]# kubectl delete -f ingress-config.yaml
ingress.networking.k8s.io "ingress-nginx" deleted
[root@m-k8s 3.3.2]# kubectl delete -f ingress-nginx.yaml
namespace "ingress-nginx" deleted







## metal lb

: 온프레미스에서 로드 벨런서 기능을 제공하는 metallb 가 있다.



[root@m-k8s 3.3.2]# kubectl create deploy lb-hname-pods --image=sysnet4admin/echo-hname
deployment.apps/lb-hname-pods created

[root@m-k8s 3.3.2]# kubectl scale deploy lb-hname-pods --replicas=3
deployment.apps/lb-hname-pods scaled

[root@m-k8s 3.3.2]# kubectl create deploy lb-ip-pods --image=sysnet4admin/echo-ip
deployment.apps/lb-ip-pods created

[root@m-k8s 3.3.2]# kubectl scale deploy lb-ip-pods --replicas=3
deployment.apps/lb-ip-pods scaled





[root@m-k8s 3.3.2]# kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
lb-hname-pods-79b95c7c7b-687q7   1/1     Running   0          114s
lb-hname-pods-79b95c7c7b-hp6kp   1/1     Running   0          2m21s
lb-hname-pods-79b95c7c7b-x2657   1/1     Running   0          114s
lb-ip-pods-6c6bb59b4-4zf5q       1/1     Running   0          65s
lb-ip-pods-6c6bb59b4-bnrx9       1/1     Running   0          22s
lb-ip-pods-6c6bb59b4-m2rzw       1/1     Running   0          22s





[root@m-k8s 3.3.4]# kubectl apply -f metallb.yaml
namespace/metallb-system created
podsecuritypolicy.policy/speaker created
serviceaccount/controller created
serviceaccount/speaker created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
role.rbac.authorization.k8s.io/config-watcher created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/config-watcher created
daemonset.apps/speaker created
deployment.apps/controller created





[root@m-k8s 3.3.4]# kubectl get pods -n metallb-system -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES
controller-5f98465b6b-b4rrm   1/1     Running   0          59s   172.16.132.20   w3-k8s   <none>           <none>
speaker-f6vkz                 1/1     Running   0          59s   192.168.1.10    m-k8s    <none>           <none>
speaker-fhkhq                 1/1     Running   0          59s   192.168.1.101   w1-k8s   <none>           <none>
speaker-pnfxg                 1/1     Running   0          59s   192.168.1.103   w3-k8s   <none>           <none>
speaker-wltxw                 1/1     Running   0          59s   192.168.1.102   w2-k8s



[root@m-k8s 3.3.4]# kubectl apply -f metallb-l2config.yaml
configmap/config created



[root@m-k8s 3.3.4]# cat metallb-l2config.yaml

```
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: nginx-ip-range
      protocol: layer2
      addresses:
      - 192.168.1.11-192.168.1.13
```



[root@m-k8s 3.3.4]# kubectl get configmap -n metallb-system
NAME     DATA   AGE
config   1      38s





[root@m-k8s 3.3.4]# kubectl expose deploy lb-hname-pods --type=LoadBalancer --name=lb-hnam-svc --port=80
service/lb-hnam-svc exposed



[root@m-k8s 3.3.4]# kubectl expose deploy lb-ip-pods --type=LoadBalancer --name=lb-ip-svc --port=80
service/lb-ip-svc exposed



[root@m-k8s 3.3.4]# kubectl get svc
NAME                TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE
hname-svc-default   ClusterIP      10.96.182.107   <none>         80/TCP,443/TCP   30m
kubernetes          ClusterIP      10.96.0.1       <none>         443/TCP          7d1h
lb-hnam-svc         LoadBalancer   10.107.91.134   192.168.1.11   80:32226/TCP     84s
lb-ip-svc           LoadBalancer   10.97.172.204   192.168.1.12   80:32505/TCP     18s



![image-20211029224330512](k8s-3장.assets/image-20211029224330512.png)



![image-20211029224319263](k8s-3장.assets/image-20211029224319263.png)





[root@m-k8s 3.3.4]# kubectl get deploy
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
lb-hname-pods   3/3     3            3           16m
lb-ip-pods      3/3     3            3           15m
[root@m-k8s 3.3.4]# kubectl delete deploy lb-hname-pods
deployment.apps "lb-hname-pods" deleted
[root@m-k8s 3.3.4]# kubectl delete deploy lb-ip-pods
deployment.apps "lb-ip-pods" deleted
[root@m-k8s 3.3.4]# kubectl get svc
NAME                TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE
hname-svc-default   ClusterIP      10.96.182.107   <none>         80/TCP,443/TCP   34m
kubernetes          ClusterIP      10.96.0.1       <none>         443/TCP          7d1h
lb-hnam-svc         LoadBalancer   10.107.91.134   192.168.1.11   80:32226/TCP     4m50s
lb-ip-svc           LoadBalancer   10.97.172.204   192.168.1.12   80:32505/TCP     3m44s
[root@m-k8s 3.3.4]# kubectl delete svc lb-ip-svc
service "lb-ip-svc" deleted
[root@m-k8s 3.3.4]# kubectl delete svc lb-hnam-svc
service "lb-hnam-svc" deleted
[root@m-k8s 3.3.4]# kubectl delete svc hname-svc-default
service "hname-svc-default" deleted







## hpa

서버에 부하가 생길때 부하량에 따라 디플로이 먼트의 파드 수를 유동적으로 관리하는 기능





[root@m-k8s 3.3.4]# kubectl create deploy hpa-hname-pods --image=sysnet4admin/echo-hname
deployment.apps/hpa-hname-pods created



[root@m-k8s 3.3.4]# kubectl expose deploy hpa-hname-pods --type=LoadBalancer --name=hpa-hname-svc --port=80
service/hpa-hname-svc exposed



[root@m-k8s 3.3.4]# kubectl get svc
NAME            TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
hpa-hname-svc   LoadBalancer   10.105.230.112   192.168.1.11   80:31038/TCP   2m40s
kubernetes      ClusterIP      10.96.0.1        <none>         443/TCP        7d1h



### top

쿠버네티스 자원이 어느 정도 사용되는지 확인하는 명령어

[root@m-k8s 3.3.4]# kubectl top pods
Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)

에러남

이유 : 자원을 요청하는 설정이 없음. hpa는 자원을 요청할 때 메트릭서버를 통해 계측  값을 전달받음

근데 여긴 메트릭 서버가 없음



https://github.com/kubernetes-sigs/metrics-server.git

에서 소스 참고



[root@m-k8s 3.3.5]# kubectl apply -f metrics-server.yaml

clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
serviceaccount/metrics-server created
deployment.apps/metrics-server created
service/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created





[root@m-k8s 3.3.5]#  kubectl top pods
NAME                              CPU(cores)   MEMORY(bytes)
hpa-hname-pods-75f874d48c-wqvc2   0m           1Mi



[root@m-k8s 3.3.5]# kubectl edit deploy hpa-hname-pods
deployment.apps/hpa-hname-pods edited

        name: echo-hname
        resources:
          requests:
            cpu: "10m"
          limits:
            cpu: "50m"





[root@m-k8s 3.3.5]# kubectl autoscale deploy hpa-hname-pods --min=1 --max=30 --cpu-percent=50



[root@m-k8s 3.3.5]# kubectl get hpa
NAME             REFERENCE                   TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-hname-pods   Deployment/hpa-hname-pods   0%/50%    1         30        1          24s



[root@m-k8s 3.3.5]# kubectl delete deploy hpa-hname-pods
deployment.apps "hpa-hname-pods" deleted



[root@m-k8s 3.3.5]# kubectl delete hpa hpa-hname-pods
horizontalpodautoscaler.autoscaling "hpa-hname-pods" deleted



[root@m-k8s 3.3.5]# kubectl delete -f metrics-server.yaml





# 쿠버네티스 오브젝트



## daemonset

: 디플로이먼트의 레플리카가 노드 수 만큼 정해져 있는 형태

- 노드 하나당 파드 한개만 생성





### -w : 오브젝트의 상태 변화 확인 옵션

kubectl get pods -n metallb-system -o wide -w





## 컨피그맵

설정을 목적으로 사용하는 오브젝트



[root@m-k8s ~]# kubectl create deploy cfgmap --image=sysnet4admin/echo-hname
deployment.apps/cfgmap created

[root@m-k8s ~]# kubectl expose deploy cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
service/cfgmap-svc exposed

[root@m-k8s ~]# kubectl get svc
NAME            TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
cfgmap-svc      LoadBalancer   10.110.110.173   192.168.1.12   80:32562/TCP   20s





아이피 범위 바꿈

[root@m-k8s 3.4.2]# vi metallb-l2config.yaml

```
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: nginx-ip-range
      protocol: layer2
      addresses:
      - 192.168.1.21-192.168.1.23
```





[root@m-k8s 3.4.2]# kubectl delete pods --all -n metallb-system
pod "controller-5f98465b6b-b4rrm" deleted
pod "speaker-f6vkz" deleted
pod "speaker-fhkhq" deleted
pod "speaker-pnfxg" deleted
pod "speaker-wltxw" deleted



그럼 새로 생성된다

[root@m-k8s 3.4.2]# kubectl get pods -n metallb-system
NAME                          READY   STATUS    RESTARTS   AGE
controller-5f98465b6b-nsplw   1/1     Running   0          60s
speaker-5967v                 1/1     Running   0          60s
speaker-7tmb6                 1/1     Running   0          60s
speaker-cmw6r                 1/1     Running   0          60s
speaker-wprnt                 1/1     Running   0          60s



서비스를 삭제하고 다시 생성

[root@m-k8s 3.4.2]# kubectl delete svc cfgmap-svc
service "cfgmap-svc" deleted
[root@m-k8s 3.4.2]# kubectl expose deploy cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
service/cfgmap-svc exposed



아이피 변경되는 것 확인

[root@m-k8s 3.4.2]# kubectl get svc
NAME            TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
cfgmap-svc      LoadBalancer   10.100.34.71     192.168.1.22   80:32392/TCP   35s







## PV, PVC

쿠버네티스에서 파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해 기록하거나 보관하는 것

모든 파드가 동일한 설정 값을 유지하기 관리하기 위해 공유된 볼륨으로 부터 공통된 설정을 가지고 올 수 있도록 설계해야 할때 사용

PVC : 지속적으로 사용가능한 볼륨 요청 - 준비된 볼륨에서 일정 공간을 할당 받는 것

PV : 지속적으로 사용가능한 볼륨 - 볼륨을 사용할 수 있게 준비하는 단계



[root@m-k8s 3.4.2]# mkdir /nfs_shared
[root@m-k8s 3.4.2]# echo '/nfs_shared  192.168.1.0/24(rw,sync,no_root_squash)' >> /etc/exports



[root@m-k8s 3.4.2]# systemctl enable --now nfs
Created symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.





[root@m-k8s 3.4.3]# kubectl apply -f nfs-pv.yaml
persistentvolume/nfs-pv created



nfs-pv.yaml

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Mi # 실제로 사용하는 양을 제한하는 것이 아니라 쓸수 있는 양을 레이블로 붙이는 것과 같다
  accessModes:
    - ReadWriteMany # PV를 어떤 방식으로 사용할지
  persistentVolumeReclaimPolicy: Retain  # PV 가 제거 될때 사용하는 방법 정의 Retain 은 유지
  nfs:
    server: 192.168.1.10
    path: /nfs_shared
```





[root@m-k8s 3.4.3]# kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
nfs-pv   100Mi      RWX            Retain           Available                                   8m44s



[root@m-k8s 3.4.3]# kubectl apply -f nfs-pvc.yaml
persistentvolumeclaim/nfs-pvc created



```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
```



[root@m-k8s 3.4.3]# kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-pvc   Bound    nfs-pv   100Mi      RWX                           67s



Bound는 pv 와 pvc가 묶여짐

pv 는 사용자가 요청할 볼륨 공간을 관리자가 만들고 pvc는 사용자 간 볼륨을 요청하는 데 사용한다는 점



[root@m-k8s 3.4.3]# kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE
nfs-pv   100Mi      RWX            Retain           Bound    default/nfs-pvc                           13m



생성한 볼륨으로 사용하는 deployment 배포

nfs-pvc-deploy.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pvc-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nfs-pvc-deploy
  template:
    metadata:
      labels:
        app: nfs-pvc-deploy
    spec:
      containers:
      - name: audit-trail
        image: sysnet4admin/audit-trail
        volumeMounts:
        - name: nfs-vol
          mountPath: /audit
      volumes:
      - name: nfs-vol
        persistentVolumeClaim:
          claimName: nfs-pvc
```





